{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff9504ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import nest_asyncio\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# Load environment variables\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the index name\n",
    "PINECONE_INDEX_NAME = \"dia\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05da8cfa",
   "metadata": {},
   "source": [
    "### Cargar Multiples URL (SCRAPING DIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a61a20a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def desde_web2(web):\n",
    "    from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "    # Create an instance of WebBaseLoader with the list of URLs\n",
    "    loader = WebBaseLoader(web)\n",
    "\n",
    "    # Set the number of requests per second to control the scraping rate\n",
    "    loader.requests_per_second = 1\n",
    "\n",
    "    # Use the aload method to asynchronously load the documents\n",
    "    # Note: You might need to handle exceptions or errors here\n",
    "    try:\n",
    "        data = loader.aload()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        data = None\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2d2293eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "\n",
    "def read_urls_from_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        # Elimina comas y comillas extras de cada línea\n",
    "        urls = [line.strip().rstrip(',').replace(\"'\", \"\").replace('\"', '') for line in file if line.strip()]\n",
    "    return urls\n",
    "\n",
    "# Assuming the 'urls' list is defined as containing the initial URLs you want to scrape\n",
    "urls = read_urls_from_file('urls.txt') # List of URLs you've already defined\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/109.0'}\n",
    "base_url = 'https://www.dia.es'\n",
    "all_unique_links = set()\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        category_segment = url.split('/')[-2]\n",
    "        # Extract and adjust links to ensure they are absolute\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            if href.startswith('/'):\n",
    "                href = base_url + href  # Converts relative links to absolute\n",
    "            # Check if URL does not contain 'sort' in the last segment\n",
    "            if href.startswith(base_url) and category_segment in href and 'sort' not in href.split('/')[-1]:\n",
    "                all_unique_links.add(href)\n",
    "\n",
    "# Convert set to list (if needed)\n",
    "all_unique_links = list(all_unique_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92623aaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4393"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_unique_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaca4b0",
   "metadata": {},
   "source": [
    "### Fragmentar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "59f455b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragmentar(data, chunk_size=500):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=60)\n",
    "    fragmentos = text_splitter.split_documents(data)\n",
    "    return fragmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c0049d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_y_fragmentar(documento):\n",
    "    \"\"\"Carga el contenido desde la web o un archivo y lo fragmenta.\"\"\"\n",
    "    if isinstance(documento, str) and documento.endswith('.txt'):\n",
    "        contenido = desde_txt(documento)\n",
    "    else:\n",
    "        contenido = desde_web2(documento)\n",
    "    \n",
    "    fragmentos = fragmentar(contenido)\n",
    "    costo_embedding(fragmentos)\n",
    "    return fragmentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0725b19a",
   "metadata": {},
   "source": [
    "### Costos OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "69bccc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def costo_embedding(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0001:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8caa61",
   "metadata": {},
   "source": [
    "### Borrando Index de Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1e52d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def borrar_indices(index_name='todos'):\n",
    "    import pinecone\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "    if index_name == 'todos':\n",
    "        indexes = pinecone.list_indexes()\n",
    "        print('Borrando todos los índices ... ')\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "        print('Listo!')\n",
    "    else:\n",
    "        print(f'Borrando el índice: {index_name} ...', end='')\n",
    "        pc.delete_index(index_name)\n",
    "        print('Listo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "71a8dae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Borrando el índice: dia ...Listo\n"
     ]
    }
   ],
   "source": [
    "borrar_indices(PINECONE_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094d5a57",
   "metadata": {},
   "source": [
    "### Creando Vectores (Embeddings) y subirlos a (Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7b2bbd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creando_vectores(index_name,fragmentos):\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model='text-embedding-ada-002')\n",
    "\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "    if index_name in pc.list_indexes().names():\n",
    "        index = pc.Index(index_name)\n",
    "        print(f'El índice {index_name} ya existe. Cargando los embeddings ... ', end='')\n",
    "        vectores = PineconeVectorStore.from_existing_index(index_name, embeddings)\n",
    "\n",
    "        \n",
    "    elif index_name not in pc.list_indexes().names():\n",
    "        print(f'Creando el índice {index_name} y los embeddings ...', end='')\n",
    "        pc.create_index(name=index_name,dimension=1536,metric=\"cosine\", spec=ServerlessSpec(cloud=\"aws\",region=\"us-east-1\"))\n",
    "        vectores = PineconeVectorStore.from_documents(fragmentos, embeddings, index_name=index_name)\n",
    "        \n",
    "        \n",
    "    return vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f1ba1",
   "metadata": {},
   "source": [
    "### Resumen Final (Dia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6025174b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##############################################################| 4393/4393 [14:46<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 3351781\n",
      "Embedding Cost in USD: 0.33518\n",
      "Total Tokens: 3351781\n",
      "Embedding Cost in USD: 0.33518\n",
      "Creando el índice dia y los embeddings ..."
     ]
    }
   ],
   "source": [
    "documento_dia = all_unique_links  # Asumimos que all_unique_links es una lista de URLs\n",
    "fragmentos_dia = cargar_y_fragmentar(documento_dia)\n",
    "costo_embedding(fragmentos_dia)\n",
    "vectores_dia = creando_vectores(PINECONE_INDEX_NAME, fragmentos_dia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30e0bf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9c960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
